{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    "<div class=\"lev1 toc-item\"><a href=\"#Exercise-2.1.-Minimizing-a-quadratic-function-and-the-curse-of-dimensionality\" data-toc-modified-id=\"Exercise-2.1.-Minimizing-a-quadratic-function-and-the-curse-of-dimensionality-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Exercise 2.1. Minimizing a quadratic function and the curse of dimensionality</a></div><div class=\"lev1 toc-item\"><a href=\"#Exercise-2.2.-Implementing-random-search-in-Python\" data-toc-modified-id=\"Exercise-2.2.-Implementing-random-search-in-Python-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exercise 2.2. Implementing random search in Python</a></div><div class=\"lev1 toc-item\"><a href=\"#Exercise-2.3.-Using-random-search-to-minimize-a-nonconvex-function\" data-toc-modified-id=\"Exercise-2.3.-Using-random-search-to-minimize-a-nonconvex-function-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exercise 2.3. Using random search to minimize a nonconvex function</a></div><div class=\"lev1 toc-item\"><a href=\"#Exercise-2.4.-Random-search-with-diminishing-steplength\" data-toc-modified-id=\"Exercise-2.4.-Random-search-with-diminishing-steplength-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Exercise 2.4. Random search with diminishing steplength</a></div><div class=\"lev1 toc-item\"><div class=\"lev1 toc-item\"><a href=\"#Exercise-2.8.-Coordinate-search-applied-to-minimize-a-simple-quadratic\" data-toc-modified-id=\"Exercise-2.8.-Coordinate-search-applied-to-minimize-a-simple-quadratic-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Exercise 2.8. Coordinate search applied to minimize a simple quadratic</a></div><div class=\"lev1 toc-item\"><a href=\"#Exercise-2.9.-Coordinate-search-with-diminishing-steplength\" data-toc-modified-id=\"Exercise-2.9.-Coordinate-search-with-diminishing-steplength-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Exercise 2.9. Coordinate search with diminishing steplength</a></div><div class=\"lev1 toc-item\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries and autograd wrapped numpy\n",
    "import autograd.numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# this is needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.1. Minimizing a quadratic function and the curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment you will verify the *curse of dimensionality* issue associated with the use of randomly sampled points for naive evaluation for the simple quadratic function\n",
    "\n",
    "$$\n",
    "g(\\mathbf{w}) = \\mathbf{w}^T\\mathbf{w}\n",
    "$$\n",
    "\n",
    "whose minimum is always $g(\\mathbf{0}_{N\\times 1}) = 0$ regardless of the input dimension $N$.\n",
    "\n",
    "Here you need to create a range of these quadratics for input dimension $N=1$ to $N = 100$.  Below you will find a Python function defining such a quadratic for general input dimension $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function defining a a simple, N dimensional quadratic function g(\\mathbf{w}) = \\mathbf{w}^T\\mathbf{w}\n",
    "def quadratic(x):\n",
    "    return np.dot(x.T,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can evaluate this function using any (real) input you like.  Some examples are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an N=1 example=> here x=np.array([1]), and quadratic(x)= 1\n",
      "an N=4 example=> here x=np.array([1,2,3,4]), and quadratic(x)= 30\n"
     ]
    }
   ],
   "source": [
    "# an example where N=1\n",
    "x=np.array([1])\n",
    "print('an N=1 example=> here x=np.array([1]), and quadratic(x)=',quadratic(x))\n",
    "\n",
    "# an example where N=4\n",
    "x = np.array([1,2,3,4])\n",
    "print('an N=4 example=> here x=np.array([1,2,3,4]), and quadratic(x)=',quadratic(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you want to sample the input space of each quadratic of this kind for dimensions $N=1,..,100$ a total of $10,000$ times each, randomly and uniformly on the hypercube $[-1,1]\\times [-1,1] \\times \\cdots \\times [-1,1]$ (this hypercube has $N$ sides).  \n",
    "\n",
    "You can produce a random number on the interval $[-1,1]$ as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.99757687])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a random number on the interval [-1,1]\n",
    "2*np.random.rand(1)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to create an $N$ dimensional version of this - you can use the general pattern below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42372693,  0.88487952,  0.7021287 , -0.47471809]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a vector of dimension N where each entry is randomly chosen on the interval [-1,1]\n",
    "N=4\n",
    "2*np.random.rand(1,4)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these components in had, generate a plot illustrating the value of this quadratics of dimension $N=100$, $N=1000$, and $N=10000$ by sampling  $100$, $1,000$, and $10,000$ times respectively and averaging. See the text of this problem for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2. Implementing random search in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a ``Python`` wrapper providing a skeleton for your production of of the random local search algorithm.  All parts marked \"TO DO\" are for you to construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random search function\n",
    "def random_search(g,alpha,max_its,w,num_samples):\n",
    "    # run random search\n",
    "    weight_history = []         # container for weight history\n",
    "    cost_history = []           # container for corresponding cost function history\n",
    "    alpha = 0\n",
    "    for k in range(1,max_its+1):                    \n",
    "        # record weights and cost evaluation\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "        \n",
    "        # construct set of random unit directions\n",
    "        ##  TO DO \n",
    "        \n",
    "        ### pick best descent direction\n",
    "        ## TO DO\n",
    "        \n",
    "        # evaluate all candidates\n",
    "        evals = np.array([g(w_val) for w_val in w_candidates])\n",
    "\n",
    "        # check directions to ensure a real descent direction to take the step in its direction\n",
    "        ## TO DO\n",
    "        \n",
    "    # record weights and cost evaluation\n",
    "    weight_history.append(w)\n",
    "    cost_history.append(g(w))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the history of function evaluations returned is called ``cost_history``.  This is because - in the context of machine learning / deep learning - mathematical functions are often referred to as *cost* or *loss* functions.\n",
    "\n",
    "When you are done, plot the cost function history associated with your run.  See the text for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.3. Using random search to minimize a nonconvex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your completed work from the previous exercises to minimize the function\n",
    "\n",
    "\\begin{equation}\n",
    "g(w_0,w_1) = \\text{tanh}(4w_0 + 4w_1) + \\text{max}(0.4w_0^2,1) + 1\n",
    "\\end{equation}\n",
    "\n",
    "using random local search again setting $P = 1000$ and 8 steps with $\\alpha = 1$ for all steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.4. Random search with diminishing steplength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the random search algorithm to minimize a famous optimization test function called the *Rosenbrock function* which takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "g\\left(w_0,w_1\\right) = 100\\left(w_1 - w_0^2\\right)^2 + \\left(w_0 - 1\\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "This function (whose contour plot is shown in the text) has a global minimum at the point $\\mathbf{w}^{\\star} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ located in a very narrow and curved valley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make the same run but use the diminishing steplength rule $\\alpha = \\frac{1}{k}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.8. Coordinate search applied to minimize a simple quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide a skeleton wrapper for coordinate search algorithm.  All parts marked \"TO DO\" are for you to construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero order coordinate search\n",
    "def coordinate_search(g,alpha,max_its,w):\n",
    "    # construct set of all coordinate directions\n",
    "    directions_plus = np.eye(np.size(w),np.size(w))\n",
    "    directions_minus = - np.eye(np.size(w),np.size(w))\n",
    "    directions = np.concatenate((directions_plus,directions_minus),axis=0)\n",
    "        \n",
    "    # run coordinate search\n",
    "    weight_history = []         # container for weight history\n",
    "    cost_history = []           # container for corresponding cost function history\n",
    "    for k in range(1,max_its+1):                    \n",
    "        # record weights and cost evaluation\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "        \n",
    "        ### pick best descent direction\n",
    "        # compute all new candidate points\n",
    "        ## TO DO\n",
    "        \n",
    "        # evaluate all candidates\n",
    "        ## TO DO\n",
    "\n",
    "        # if we find a real descent direction take the step in its direction\n",
    "         ## TO DO \n",
    "            \n",
    "    # record weights and cost evaluation\n",
    "    weight_history.append(w)\n",
    "    cost_history.append(g(w))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare 5 steps of the random search algorithm (with $P = 1000$ random directions tested at each step) to 7 steps of coordinate search, using the same starting point $\\mathbf{w}^0 = \\begin{bmatrix}3 \\\\ 4\\end{bmatrix}$ and fixed steplength parameter value $\\alpha = 1$ for both.  \n",
    "\n",
    "The test function in this case is the simple quadratic used in several of the examples of the previous Section\n",
    "\n",
    "\\begin{equation}\n",
    "g(w_0,w_1) = w_0^2 + w_1^2 + 2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.9. Coordinate search with diminishing steplength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare 5 steps of the random search algorithm (with $P = 1000$ random directions tested at each step) to 5 steps of coordinate search, using the same starting point $\\mathbf{w}^0 = \\begin{bmatrix}3 \\\\ 4\\end{bmatrix}$ and fixed steplength parameter value $\\alpha = 1$ for both.  The test function in this case is a skewed quadratic function\n",
    "\n",
    "\\begin{equation}\n",
    "g(w_0,w_1) = 0.26\\left(w_0^2 + w_1^2\\right) - 0.48w_0w_1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By making the steplength parameter smaller we can encourage coordinate search to find its way towards the function's minimum at the origin.  Re-run coordinate search using a diminishing steplength parameter $\\alpha = \\frac{1}{k}$ at the $k^{th}$ step for $1000$ steps.  With this run the method gets much closer to the function minimum.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "216.233px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "631.733px",
    "left": "0px",
    "right": "1228px",
    "top": "116.267px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
